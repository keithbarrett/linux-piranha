<HTML
><HEAD
><TITLE
>        FOS Architecture
      </TITLE
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.54"><LINK
REL="HOME"
TITLE="piranha"
HREF="index.html"><LINK
REL="UP"
TITLE="      Failover Services (FOS)
    "
HREF="ch-fos.html"><LINK
REL="PREVIOUS"
TITLE="      Failover Services (FOS)
    "
HREF="ch-fos.html"><LINK
REL="NEXT"
TITLE="        Setting up an FOS Cluster
      "
HREF="x2670.html"></HEAD
><BODY
CLASS="SECT1"
BGCOLOR="#FFFFFF"
TEXT="#000000"
LINK="#0000FF"
VLINK="#840084"
ALINK="#0000FF"
><DIV
CLASS="NAVHEADER"
><TABLE
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TH
COLSPAN="3"
ALIGN="center"
>piranha  </TH
></TR
><TR
><TD
WIDTH="10%"
ALIGN="left"
VALIGN="bottom"
><A
HREF="ch-fos.html"
>Prev</A
></TD
><TD
WIDTH="80%"
ALIGN="center"
VALIGN="bottom"
>  Failover Services (FOS)</TD
><TD
WIDTH="10%"
ALIGN="right"
VALIGN="bottom"
><A
HREF="x2670.html"
>Next</A
></TD
></TR
></TABLE
><HR
ALIGN="LEFT"
WIDTH="100%"></DIV
><DIV
CLASS="SECT1"
><H1
CLASS="SECT1"
><A
NAME="AEN2452"
>FOS Architecture</A
></H1
><P
>        While the architecture of an FOS cluster consists of relatively few
        components, it is important to completely understand how these
        components work together to provide highly-available services.  This
        section discusses FOS in more detail.
      </P
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2461"
>Primary and Backup Nodes</A
></H2
><P
>          A node in an FOS cluster is defined as being either a
          <I
CLASS="FIRSTTERM"
>primary</I
> or a <I
CLASS="FIRSTTERM"
>backup</I
>
          system.  The two types operate identically except in two situations:
        </P
><P
></P
><UL
><LI
><P
>              In cases where both nodes of a two node cluster attempt to declare
              themselves as actively providing services (a <I
CLASS="FIRSTTERM"
>quorum
              tie</I
>), the primary node will always win the stalemate
              and force the backup system to become an inactive standby.
            </P
></LI
><LI
><P
>              An FOS cluster will always have a primary node; there are no
              configuration options to eliminate it. If a single-node cluster is
              defined (a configuration allowed by Piranha for use with other
              components, but of no value in an FOS setup), then that single
              node must be configured as a primary. Note that this is not the
              same situation as when a two node cluster has one node down
              &#8212; in that case the remaining node will be either a primary
              or backup, depending on which node failed, and which is still
              running.
            </P
></LI
></UL
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2490"
>Cluster Node States</A
></H2
><P
>          All nodes in a running FOS cluster will be operating in one of the
          following states at any point in time:
        </P
><DIV
CLASS="TABLE"
><A
NAME="FOS-STATES"
></A
><P
><B
>Table 7-1. FOS Node States</B
></P
><TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
CELLSPACING="0"
CELLPADDING="4"
CLASS="CALSTABLE"
><THEAD
><TR
><TH
ALIGN="LEFT"
VALIGN="TOP"
>                  State
                </TH
><TH
ALIGN="LEFT"
VALIGN="TOP"
>                  Description
                </TH
></TR
></THEAD
><TBODY
><TR
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  Active
                </TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  The node is providing the configured IP services to the public
                  users. Only one node in an FOS cluster is allowed to be the
                  active node at any point in time.
                </TD
></TR
><TR
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  Inactive
                </TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  The node is acting as a standby system while the other node
                  (sometimes referred to as its <I
CLASS="FIRSTTERM"
>partner</I
>)
                  is active. The inactive node monitors the services on the
                  active node, and will become the active node if it detects
                  one of those services failing to respond.
                </TD
></TR
><TR
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  Dead
                </TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  The node is down, or its services are non-responsive.
                </TD
></TR
></TBODY
></TABLE
></DIV
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2517"
>Heartbeats</A
></H2
><P
>          Each of the cluster nodes send a periodic heartbeat message to the
          network, with an indication of whether that node is currently active
          or inactive. Each node expects to see a heartbeat message from its
          partner. If it is not received, this is considered a failure of that
          system and may result in a failover of services.  This test is
          independent of the IP service monitoring.
        </P
><P
>          When the inactive node fails to see the heartbeat of the active node,
          it treats the missing heartbeat as indicating a cluster failure, and
          will perform a failover of all services. If the active node fails to
          see a heartbeat from the inactive node, the inactive system is logged
          as being unavailable for failover, while the services continue normal
          operation on the active node.
        </P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2527"
>Virtual IP (VIP) Addresses</A
></H2
><P
>          Failover in an FOS cluster is accomplished through the use of VIP
          (Virtual IP) addresses. They are virtual because they exist as
          additional IP addresses to the node's regular host IP address.  In
          other words, a node can have multiple IP addresses, all on the same
          network interface.  A node can be accessed by its VIP address(es) as
          well as by its regular host address.
        </P
><P
>          VIP addresses are a feature of Linux and can be defined on any network
          interface present. For FOS, the VIP addresses and their network
          interfaces have to be accessible by the clients on the public network.
        </P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2544"
>Services</A
></H2
><P
>          Each service defined in FOS requires a VIP address, a port number, a
          start command (or script name), and a stop/shutdown command (or script
          name). Each service can be defined as having a different VIP address,
          or some (or all) can use the same VIP address.  Services currently
          cannot failover individually &#8212; when one service fails, they all
          failover to the inactive system. This means that in most cases there
          is little value in specifying individual VIP addresses for
          services. However, there are some cases where this may be desirable:
        </P
><P
></P
><UL
><LI
><P
>              You have an existing environment where the IP services are already
              being provided by different servers, and you are using FOS to
              consolidate them to a single, more fault-tolerant cluster. In this
              case, using their previous IP addresses as VIP addresses will
              allow you to migrate without needing to change any client systems.
            </P
></LI
><LI
><P
>              You anticipate long-term growth in the use of each service.  Using
              different VIP addresses now may make it easier to migrate the
              individual services to separate, dedicated FOS clusters in the
              future, while reducing the possible impact of changes on client
              systems.
            </P
></LI
></UL
><P
>          In general, however, it is recommended that you use the same VIP
          address for all FOS services.  Because a single VIP address means only
          one VIP address must be moved from the active node to the inactive
          node during failover, a single VIP address means faster, more reliable
          failovers.
        </P
><P
>          Each service is also allowed two optional parameters: a
          <I
CLASS="FIRSTTERM"
>send</I
> string and an
          <I
CLASS="FIRSTTERM"
>expect</I
> string. If specified, these strings will
          be used as part of the service monitoring that will test whether the
          service is actually responding. If they are not specified, the service
          will be considered functional if a socket connection attempt to it
          succeeds.
        </P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="S2-FOS-SERVICE-MON"
>Service Monitoring</A
></H2
><P
>          On the inactive node, a monitoring daemon is run for each FOS service
          on the active node. Each monitoring daemon, called
          <TT
CLASS="APPLICATION"
>nanny</TT
>, periodically tests a service on the
          active node.  The test goes through the following steps:
        </P
><P
></P
><UL
><LI
><P
>              The first test is whether a connection to that service's tcp/ip
              port is successful or not. If an error results, that service is
              considered dysfunctional and a failover occurs.  Otherwise, the
              test continues.
            </P
></LI
><LI
><P
>              If it has been supplied, a character string (the send string) is
              sent to service's port.  If an error occurs, the service is
              considered dysfunctional, and a failover occurs.  Otherwise, the
              test continues.
            </P
></LI
><LI
><P
>              If an expect string is supplied, an attempt to receive a response
              takes place.  If an error occurs, a response is not received, or
              the response does not match the expect string, the service is
              considered dysfunctional, and a failover occurs.  Otherwise, the
              service is considered functional.
            </P
></LI
></UL
><P
>          When <TT
CLASS="APPLICATION"
>nanny</TT
> monitors a service, it connects
          using the active node's host IP address rather than the VIP address of
          the service. This is done to ensure cluster reliability.  There are
          windows during service failure (and the subsequent failover) where the
          VIP address may exist on both cluster nodes, or be missing
          altogether. Using the host IP address instead of the VIP address to
          monitor a service ensures that the correct system is always being
          examined and tested.
        </P
><P
>          The following diagram illustrates the service monitoring logic used by
          FOS:
        </P
><DIV
CLASS="FIGURE"
><P
><IMG
SRC="./figs/fos/svc-mon-flow.gif"></P
><P
><B
>Figure 7-1.             Service Monitoring Logic
          </B
></P
></DIV
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2585"
>Failover</A
></H2
><P
>          FOS automatically creates, deletes, or moves VIP addresses based on
          the information in its configuration file. Each time FOS changes a VIP
          address, ARP broadcasts are sent out to inform the connected network
          that the MAC address for the VIP address has changed. If an end-user
          accesses a service by referring to its VIP address and port, it will
          be transparent which system is actually providing that service.
        </P
><P
>          In normal operation, an FOS system will have one active node with
          running services (and their associated VIP addresses), and an inactive
          node monitoring the services on the active node. This is illustrated
          below:
        </P
><DIV
CLASS="FIGURE"
><P
><IMG
SRC="./figs/fos/before-failover.gif"></P
><P
><B
>Figure 7-2.             Running FOS Cluster Before Failover
          </B
></P
></DIV
><P
>          When a failover occurs, the service VIP addresses are recreated on the
          inactive node, and the inactive node becomes active by starting the
          services. The originally-active system is notified (by heartbeat) that
          it should become inactive (if possible, depending on the failure
          situation). If it does go inactive, it will stop all services, start
          the monitoring programs, and become eligible for a failover should the
          new active system suffer an outage. This is illustrated below:
        </P
><DIV
CLASS="FIGURE"
><P
><IMG
SRC="./figs/fos/after-failover.gif"></P
><P
><B
>
Figure 7-3.             Running FOS Cluster After Failover
</B
></P
></DIV
><P
>
          If, for some reason, the services on the originally-active system
          cannot be stopped, it does not interfere with the cluster, because the
          VIP addresses have been moved to the new active system, directing all
          traffic away from the originally-active system.
        </P
></DIV
><DIV
CLASS="SECT2"
><H2
CLASS="SECT2"
><A
NAME="AEN2603"
>Components</A
></H2
><P
>          An FOS system consists of the following components:
        </P
><DIV
CLASS="TABLE"
><A
NAME="FOS-PARTS"
></A
><P
><B
>Table 7-2. FOS Components</B
></P
><TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
CELLSPACING="0"
CELLPADDING="4"
CLASS="CALSTABLE"
><THEAD
><TR
><TH
ALIGN="LEFT"
VALIGN="TOP"
>                  Component
                </TH
><TH
ALIGN="LEFT"
VALIGN="TOP"
>                  Description
                </TH
></TR
></THEAD
><TBODY
><TR
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  Piranha Web Interface
                </TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  A graphical interface for creating and maintaining the cluster
                  configuration file. (Please read <A
HREF="ch-web-interface.html"
>web interface chapter </A
> for more information on the
                  Piranha Web Interface.)
                </TD
></TR
><TR
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  <TT
CLASS="FILENAME"
>/etc/sysconfig/ha/lvs.cf</TT
>
                </TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  The cluster configuration file. Can be any filename desired;
                  this is the default. The FOS-related contents of this file are
                  detailed later in this document.
                </TD
></TR
><TR
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  <TT
CLASS="FILENAME"
>/usr/sbin/pulse</TT
>
                </TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  Main Piranha program and daemon process. Provides and tests
                  for a heartbeat between the cluster nodes.  Also starts and
                  stops the <TT
CLASS="APPLICATION"
>fos</TT
> daemon process as
                  needed.
                </TD
></TR
><TR
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  <TT
CLASS="FILENAME"
>/etc/rc.d/init.d/pulse</TT
>
                </TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  Start and stop script for the <TT
CLASS="APPLICATION"
>pulse</TT
>
                  program.
                </TD
></TR
><TR
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  <TT
CLASS="FILENAME"
>/usr/sbin/fos</TT
>
                </TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  Main FOS program and daemon. Started by
                  <TT
CLASS="APPLICATION"
>pulse</TT
>, this program operates in two
                  modes. On the active node, it is started using a
                  <B
CLASS="COMMAND"
>--active</B
> option which causes it to
                  automatically start and stop the IP service(s). On the
                  inactive node, it is started with a
                  <B
CLASS="COMMAND"
>--monitor</B
> option which causes it to start
                  and stop the <TT
CLASS="APPLICATION"
>nanny</TT
> service
                  monitoring daemon(s).

                  When a failure is detected by the inactive node, the
                  <TT
CLASS="APPLICATION"
>fos</TT
> daemon initiates a failover by
                  exiting, which in turn causes <TT
CLASS="APPLICATION"
>pulse</TT
>
                  to restart it using the <B
CLASS="COMMAND"
>--active</B
> option,
                  and to notify the partner cluster node that it is to go
                  inactive.
                </TD
></TR
><TR
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  <TT
CLASS="FILENAME"
>/usr/sbin/nanny</TT
>
                </TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  Service monitoring program and daemon. Started by
                  <TT
CLASS="APPLICATION"
>fos</TT
>, there is one
                  <TT
CLASS="APPLICATION"
>nanny</TT
> daemon for each defined
                  service to monitor.  The <TT
CLASS="APPLICATION"
>nanny</TT
>
                  processes only run on the inactive system, and monitor the
                  services on the active system for failure. If a failure is
                  detected, the <TT
CLASS="APPLICATION"
>nanny</TT
> daemon notifies
                  the <TT
CLASS="APPLICATION"
>fos</TT
> daemon of the failure by
                  exiting, which in turn causes <TT
CLASS="APPLICATION"
>fos</TT
>
                  to terminate all other <TT
CLASS="APPLICATION"
>nanny</TT
>
                  processes.  Then <TT
CLASS="APPLICATION"
>fos</TT
> exits to
                  notify the <TT
CLASS="APPLICATION"
>pulse</TT
> daemon that a
                  failure has occurred.
                </TD
></TR
><TR
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  <TT
CLASS="FILENAME"
>/usr/sbin/send_arp</TT
>
                </TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
>                  Program used by <TT
CLASS="APPLICATION"
>fos</TT
> to broadcast to
                  the public network which system is currently providing the
                  service for a VIP address.
                </TD
></TR
></TBODY
></TABLE
></DIV
><P
>          The components on a running FOS system supporting two services looks
          like this:
        </P
><DIV
CLASS="FIGURE"
><P
><IMG
SRC="./figs/fos/components.gif"></P
><P
><B
>Figure 7-4.             Components of a Running FOS Cluster
          </B
></P
></DIV
></DIV
></DIV
><DIV
CLASS="NAVFOOTER"
><HR
ALIGN="LEFT"
WIDTH="100%"><TABLE
WIDTH="100%"
BORDER="0"
CELLPADDING="0"
CELLSPACING="0"
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
><A
HREF="ch-fos.html"
>Prev</A
></TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="index.html"
>Home</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
><A
HREF="x2670.html"
>Next</A
></TD
></TR
><TR
><TD
WIDTH="33%"
ALIGN="left"
VALIGN="top"
>Failover Services (FOS)</TD
><TD
WIDTH="34%"
ALIGN="center"
VALIGN="top"
><A
HREF="ch-fos.html"
>Up</A
></TD
><TD
WIDTH="33%"
ALIGN="right"
VALIGN="top"
>Setting up an FOS Cluster</TD
></TR
></TABLE
></DIV
></BODY
></HTML
>